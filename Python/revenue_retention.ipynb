{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort Revenue Retention Analysis\n",
    "\n",
    "In this notebook we study an alternative approach for the cohort analysis problem presented in [A Simple Cohort Retention Analysis in PyMC](https://juanitorduz.github.io/retention/). Instead of using a linear model to estimate the retention rate, we use a Bayesian Additive Regression Tree (BART) model(see [`pymc-bart`](https://www.pymc.io/projects/bart/en/latest/)). The BART model is a flexible non-parametric model that can be used to model complex relationships between the response and the predictors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pymc_bart as pmb\n",
    "import pymc.sampling_jax\n",
    "import pytensor.tensor as pt\n",
    "import seaborn as sns\n",
    "from scipy.special import logit\n",
    "from sklearn.preprocessing import MaxAbsScaler, LabelEncoder\n",
    "\n",
    "\n",
    "plt.style.use(\"bmh\")\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 7]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = sum(map(ord, \"retention\"))\n",
    "rng: np.random.Generator = np.random.default_rng(seed=seed)\n",
    "random_seed_int: int = rng.integers(low=0, high=100, size=1).item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Here we simply read the data from the previous post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"../data/retention_data.csv\", parse_dates=[\"cohort\", \"period\"])\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"revenue_per_users\"] = data_df[\"revenue\"] / data_df[\"n_users\"]\n",
    "data_df[\"revenue_per_active_users\"] = data_df[\"revenue\"] / data_df[\"n_active_users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.query(\"revenue_per_active_users.isna()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.fillna(value={\"revenue_per_active_users\": 0.0}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a data train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_train_test_split = \"2022-11-01\"\n",
    "\n",
    "train_data_df = data_df.query(\"period <= @period_train_test_split\")\n",
    "test_data_df = data_df.query(\"period > @period_train_test_split\")\n",
    "test_data_df = test_data_df[\n",
    "    test_data_df[\"cohort\"].isin(train_data_df[\"cohort\"].unique())\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "For a detailed EDA of the data, please refer to the previous post. Here we simply display the most important plots. First, here is the retention matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "fmt = lambda y, _: f\"{y :0.0%}\"\n",
    "\n",
    "(\n",
    "    train_data_df.assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\"),\n",
    "        period=lambda df: df[\"period\"].dt.strftime(\"%Y-%m\"),\n",
    "    )\n",
    "    .query(\"cohort_age != 0\")\n",
    "    .filter([\"cohort\", \"period\", \"retention\"])\n",
    "    .pivot(index=\"cohort\", columns=\"period\", values=\"retention\")\n",
    "    .pipe(\n",
    "        (sns.heatmap, \"data\"),\n",
    "        cmap=\"viridis_r\",\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"black\",\n",
    "        annot=True,\n",
    "        fmt=\"0.0%\",\n",
    "        cbar_kws={\"format\": mtick.FuncFormatter(fmt)},\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_title(\"Retention by Cohort and Period\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot the retention rate by cohort over time (period):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "sns.lineplot(\n",
    "    x=\"period\",\n",
    "    y=\"retention\",\n",
    "    hue=\"cohort\",\n",
    "    palette=\"viridis_r\",\n",
    "    alpha=0.8,\n",
    "    data=train_data_df.query(\"cohort_age > 0\").assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\")\n",
    "    ),\n",
    "    ax=ax,\n",
    ")\n",
    "ax.legend(title=\"cohort\", loc=\"center left\", bbox_to_anchor=(1, 0.5), fontsize=7.5)\n",
    "ax.set(title=\"Retention by Cohort and Period\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "fmt = lambda y, _: f\"{y :0.0f}\"\n",
    "\n",
    "(\n",
    "    train_data_df.assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\"),\n",
    "        period=lambda df: df[\"period\"].dt.strftime(\"%Y-%m\"),\n",
    "    )\n",
    "    .query(\"cohort_age != 0\")\n",
    "    .filter([\"cohort\", \"period\", \"n_active_users\"])\n",
    "    .pivot(index=\"cohort\", columns=\"period\", values=\"n_active_users\")\n",
    "    .pipe(\n",
    "        (sns.heatmap, \"data\"),\n",
    "        cmap=\"viridis_r\",\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"black\",\n",
    "        annot=True,\n",
    "        annot_kws={\"fontsize\":8},\n",
    "        fmt=\"0.0f\",\n",
    "        cbar_kws={\"format\": mtick.FuncFormatter(fmt)},\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_title(\"Active Users by Cohort and Period\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "fmt = lambda y, _: f\"{y :0.0f}\"\n",
    "\n",
    "(\n",
    "    train_data_df.assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\"),\n",
    "        period=lambda df: df[\"period\"].dt.strftime(\"%Y-%m\"),\n",
    "    )\n",
    "    .query(\"cohort_age != 0\")\n",
    "    .filter([\"cohort\", \"period\", \"revenue\"])\n",
    "    .pivot(index=\"cohort\", columns=\"period\", values=\"revenue\")\n",
    "    .pipe(\n",
    "        (sns.heatmap, \"data\"),\n",
    "        cmap=\"viridis_r\",\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"black\",\n",
    "        annot=True,\n",
    "        annot_kws={\"fontsize\": 6},\n",
    "        fmt=\"0.0f\",\n",
    "        cbar_kws={\"format\": mtick.FuncFormatter(fmt)},\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_title(\"Revenue by Cohort and Period\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "fmt = lambda y, _: f\"{y :0.0f}\"\n",
    "\n",
    "(\n",
    "    train_data_df.assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\"),\n",
    "        period=lambda df: df[\"period\"].dt.strftime(\"%Y-%m\"),\n",
    "    )\n",
    "    .query(\"cohort_age != 0\")\n",
    "    .filter([\"cohort\", \"period\", \"revenue_per_active_users\"])\n",
    "    .pivot(index=\"cohort\", columns=\"period\", values=\"revenue_per_active_users\")\n",
    "    .pipe(\n",
    "        (sns.heatmap, \"data\"),\n",
    "        cmap=\"viridis_r\",\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"black\",\n",
    "        annot=True,\n",
    "        annot_kws={\"fontsize\": 9},\n",
    "        fmt=\"0.0f\",\n",
    "        cbar_kws={\"format\": mtick.FuncFormatter(fmt)},\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_title(\"Revenue by Cohort and Period\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "fmt = lambda y, _: f\"{y :0.0f}\"\n",
    "\n",
    "(\n",
    "    train_data_df.assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\"),\n",
    "        period=lambda df: df[\"period\"].dt.strftime(\"%Y-%m\"),\n",
    "    )\n",
    "    .query(\"cohort_age != 0\")\n",
    "    .filter([\"cohort\", \"period\", \"revenue_per_users\"])\n",
    "    .pivot(index=\"cohort\", columns=\"period\", values=\"revenue_per_users\")\n",
    "    .pipe(\n",
    "        (sns.heatmap, \"data\"),\n",
    "        cmap=\"viridis_r\",\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"black\",\n",
    "        annot=True,\n",
    "        annot_kws={\"fontsize\": 9},\n",
    "        fmt=\"0.0f\",\n",
    "        cbar_kws={\"format\": mtick.FuncFormatter(fmt)},\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_title(\"Revenue by Cohort and Period\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the previous post, we see the following:\n",
    "\n",
    "* It seems that for a given period, the retention rates of the new cohorts are higher than the retention rates of the older cohorts. This is a clear indication that the retention rate is a function of the absolute cohort age.\n",
    "* We also see a clear seasonality component in the retention rates.\n",
    "* For a given cohort, seasonality peaks are decreasing as a function of time (period)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model we want to test is the following:\n",
    "\n",
    "\\begin{align*}\n",
    "N_{\\text{active}} & \\sim \\text{Binomial}(N_{\\text{total}}, p) \\\\\n",
    "\\textrm{logit}(p) & = \\text{BART}(\\text{cohort age}, \\text{age}, \\text{month})\n",
    "\\end{align*}\n",
    "\n",
    "That is, we want to use a BART model to estimate the retention rate as a function of the absolute cohort age, the cohort age and the month. Note that we do not need to specify the relation between variables, the BART model will learn it from the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations\n",
    "\n",
    "We do similar transformations to the data as for the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(float).eps\n",
    "train_data_red_df = train_data_df.query(\"cohort_age > 0\").reset_index(drop=True)\n",
    "train_obs_idx = train_data_red_df.index.to_numpy()\n",
    "train_n_users = train_data_red_df[\"n_users\"].to_numpy()\n",
    "train_n_active_users = train_data_red_df[\"n_active_users\"].to_numpy()\n",
    "train_retention = train_data_red_df[\"retention\"].to_numpy()\n",
    "train_retention_logit = logit(train_retention + eps)\n",
    "train_data_red_df[\"month\"] = train_data_red_df[\"period\"].dt.strftime(\"%m\").astype(int)\n",
    "train_data_red_df[\"cohort_month\"] = (\n",
    "    train_data_red_df[\"cohort\"].dt.strftime(\"%m\").astype(int)\n",
    ")\n",
    "train_data_red_df[\"period_month\"] = (\n",
    "    train_data_red_df[\"period\"].dt.strftime(\"%m\").astype(int)\n",
    ")\n",
    "train_revenue = train_data_red_df[\"revenue\"].to_numpy() + eps\n",
    "train_revenue_per_user = train_revenue / train_n_active_users\n",
    "\n",
    "train_cohort = train_data_red_df[\"cohort\"].to_numpy()\n",
    "train_cohort_encoder = LabelEncoder()\n",
    "train_cohort_idx = train_cohort_encoder.fit_transform(train_cohort).flatten()\n",
    "train_period = train_data_red_df[\"period\"].to_numpy()\n",
    "train_period_encoder = LabelEncoder()\n",
    "train_period_idx = train_period_encoder.fit_transform(train_period).flatten()\n",
    "\n",
    "features: list[str] = [\"age\", \"cohort_age\", \"month\"]\n",
    "x_train = train_data_red_df[features]\n",
    "\n",
    "train_age = train_data_red_df[\"age\"].to_numpy()\n",
    "train_age_scaler = MaxAbsScaler()\n",
    "train_age_scaled = train_age_scaler.fit_transform(train_age.reshape(-1, 1)).flatten()\n",
    "train_cohort_age = train_data_red_df[\"cohort_age\"].to_numpy()\n",
    "train_cohort_age_scaler = MaxAbsScaler()\n",
    "train_cohort_age_scaled = train_cohort_age_scaler.fit_transform(\n",
    "    train_cohort_age.reshape(-1, 1)\n",
    ").flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification\n",
    "\n",
    "We strongly recommend to take a look into the [BART Overview](https://www.pymc.io/projects/bart/en/latest/examples/BART_introduction.html) section presented in the `pymc-bart` documentation. One thing to note is that in all examples presented, the BART model is used to model the expected value of the response variable. In our case, we have a Binomial likelihood, so we need to apply the logit link function to the BART component (Thank you [Osvaldo A Martin](https://github.com/aloctavodia) for the support and indication on some numerical tips and tricks!, see [this issue](https://github.com/pymc-devs/pymc-bart/issues/31)). Let's see how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"feature\": features}) as model:\n",
    "\n",
    "    # --- Data ---\n",
    "    model.add_coord(name=\"obs\", values=train_obs_idx, mutable=True)\n",
    "    age_scaled = pm.MutableData(name=\"age_scaled\", value=train_age_scaled, dims=\"obs\")\n",
    "    cohort_age_scaled = pm.MutableData(\n",
    "        name=\"cohort_age_scaled\", value=train_cohort_age_scaled, dims=\"obs\"\n",
    "    )\n",
    "    x = pm.MutableData(name=\"x\", value=x_train, dims=(\"obs\", \"feature\"))\n",
    "    n_users = pm.MutableData(name=\"n_users\", value=train_n_users, dims=\"obs\")\n",
    "    n_active_users = pm.MutableData(\n",
    "        name=\"n_active_users\", value=train_n_active_users, dims=\"obs\"\n",
    "    )\n",
    "    revenue = pm.MutableData(name=\"revenue\", value=train_revenue, dims=\"obs\")\n",
    "\n",
    "    # --- Priors ---\n",
    "    intercept = pm.Normal(name=\"intercept\", mu=0, sigma=1)\n",
    "    b_age_scaled = pm.Normal(name=\"b_age_scaled\", mu=0, sigma=1)\n",
    "    b_cohort_age_scaled = pm.Normal(name=\"b_cohort_age_scaled\", mu=0, sigma=1)\n",
    "    b_age_cohort_age_interaction = pm.Normal(\n",
    "        name=\"b_age_cohort_age_interaction\", mu=0, sigma=1\n",
    "    )\n",
    "\n",
    "    # --- Parametrization ---\n",
    "    # The BART component models the image of the retention rate under the\n",
    "    # logit transform so that the range is not constrained to [0, 1].\n",
    "    mu = pmb.BART(name=\"mu\", X=x, Y=train_retention_logit, m=50, dims=\"obs\")\n",
    "    # We use the inverse logit transform to get the retention rate back into [0, 1].\n",
    "    p = pm.Deterministic(name=\"p\", var=pm.math.invlogit(mu), dims=\"obs\")\n",
    "    # We add a small epsilon to avoid numerical issues.\n",
    "    p = pt.switch(pt.eq(p, 0), eps, p)\n",
    "    p = pt.switch(pt.eq(p, 1), 1 - eps, p)\n",
    "\n",
    "    # For the revenue component we use a Gamma distribution where we combine the number\n",
    "    # of estimated active users with the average revenue per user.\n",
    "    lam_log = pm.Deterministic(\n",
    "        name=\"lam_log\",\n",
    "        var=intercept\n",
    "        + b_age_scaled * age_scaled\n",
    "        + b_cohort_age_scaled * cohort_age_scaled\n",
    "        + b_age_cohort_age_interaction * age_scaled * cohort_age_scaled,\n",
    "        dims=\"obs\",\n",
    "    )\n",
    "\n",
    "    lam = pm.Deterministic(name=\"lam\", var=pm.math.exp(lam_log), dims=\"obs\")\n",
    "\n",
    "    # --- Likelihood ---\n",
    "    n_active_users_estimated = pm.Binomial(\n",
    "        name=\"n_active_users_estimated\",\n",
    "        n=n_users,\n",
    "        p=p,\n",
    "        observed=n_active_users,\n",
    "        dims=\"obs\",\n",
    "    )\n",
    "\n",
    "    x = pm.Gamma(\n",
    "        name=\"revenue_estimated\",\n",
    "        alpha=n_active_users_estimated + eps,\n",
    "        beta=lam,\n",
    "        observed=revenue,\n",
    "        dims=\"obs\",\n",
    "    )\n",
    "\n",
    "    mean_revenue_per_user = pm.Deterministic(\n",
    "        name=\"mean_revenue_per_user\", var=(1 / lam), dims=\"obs\"\n",
    "    )\n",
    "    pm.Deterministic(\n",
    "        name=\"mean_revenue_per_active_user\", var=p * mean_revenue_per_user, dims=\"obs\"\n",
    "    )\n",
    "\n",
    "pm.model_to_graphviz(model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "Now we are ready to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata = pm.sample(draws=2_000, chains=4, random_seed=rng)\n",
    "    posterior_predictive = pm.sample_posterior_predictive(trace=idata, random_seed=rng)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Diagnostics\n",
    "\n",
    "We look into the posterior predictive check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_ppc(\n",
    "    data=posterior_predictive,\n",
    "    kind=\"cumulative\",\n",
    "    observed_rug=True,\n",
    "    random_seed=random_seed_int,\n",
    ")\n",
    "ax[0].set(\n",
    "    title=\"Posterior Predictive Check (Retention)\",\n",
    "    xscale=\"log\",\n",
    "    xlabel=\"likelihood (n_active_users) - log scale\",\n",
    ")\n",
    "ax[1].set(\n",
    "    title=\"Posterior Predictive Check (Revenue)\",\n",
    "    xscale=\"log\",\n",
    "    xlabel=\"likelihood (revenue) - log scale\",\n",
    "    xlim=(1, None), # to avoid plotting the clipped value `eps`.\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata.sample_stats[\"diverging\"].sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = az.plot_trace(\n",
    "    data=idata,\n",
    "    var_names=[\n",
    "        \"intercept\",\n",
    "        \"b_age_scaled\",\n",
    "        \"b_cohort_age_scaled\",\n",
    "        \"b_age_cohort_age_interaction\",\n",
    "    ],\n",
    "    compact=True,\n",
    "    kind=\"rank_bars\",\n",
    "    backend_kwargs={\"figsize\": (12, 10), \"layout\": \"constrained\"},\n",
    ")\n",
    "plt.gcf().suptitle(\"Model Trace\", fontsize=16);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be doing a good job ðŸ™‚ ! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention Rate In-Sample Predictions\n",
    "\n",
    "Let's see how the model performs in-sample. We plot the retention rate posterior mean predictions for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posterior_retention = (\n",
    "    posterior_predictive.posterior_predictive[\"n_active_users_estimated\"]\n",
    "    / train_n_users[np.newaxis, None]\n",
    ")\n",
    "train_posterior_retention_mean = az.extract(\n",
    "    data=train_posterior_retention, var_names=[\"n_active_users_estimated\"]\n",
    ").mean(\"sample\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "sns.scatterplot(\n",
    "    x=\"retention\",\n",
    "    y=\"posterior_retention_mean\",\n",
    "    data=train_data_red_df.assign(\n",
    "        posterior_retention_mean=train_posterior_retention_mean\n",
    "    ),\n",
    "    hue=\"age\",\n",
    "    palette=\"viridis_r\",\n",
    "    size=\"n_users\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.axline(xy1=(0.3, 0.3), slope=1, color=\"black\", linestyle=\"--\", label=\"diagonal\")\n",
    "ax.legend()\n",
    "ax.set(title=\"Posterior Predictive - Retention Mean\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posterior_revenue_mean = az.extract(\n",
    "    data=posterior_predictive,\n",
    "    group=\"posterior_predictive\",\n",
    "    var_names=[\"revenue_estimated\"],\n",
    ").mean(\"sample\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "sns.scatterplot(\n",
    "    x=\"revenue\",\n",
    "    y=\"posterior_revenue_mean\",\n",
    "    data=train_data_red_df.assign(posterior_revenue_mean=train_posterior_revenue_mean),\n",
    "    hue=\"age\",\n",
    "    palette=\"viridis_r\",\n",
    "    size=\"n_users\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.axline(xy1=(1e5, 1e5), slope=1, color=\"black\", linestyle=\"--\", label=\"diagonal\")\n",
    "ax.legend()\n",
    "ax.set(\n",
    "    title=\"Posterior Predictive - Revenue Mean\",\n",
    "    xscale=\"log\",\n",
    "    yscale=\"log\",\n",
    "    xlabel=\"revenue (log)\",\n",
    "    ylabel=\"posterior_revenue_mean (log)\",\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look into the uncertainty estimates for a subset of individual cohorts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retention_hdi = az.hdi(ary=train_posterior_retention)[\"n_active_users_estimated\"]\n",
    "\n",
    "\n",
    "def plot_train_retention_hdi_cohort(cohort_index: int, ax: plt.Axes) -> plt.Axes:\n",
    "\n",
    "    mask = train_cohort_idx == cohort_index\n",
    "\n",
    "    ax.fill_between(\n",
    "        x=train_period[train_period_idx[mask]],\n",
    "        y1=train_retention_hdi[mask, :][:, 0],\n",
    "        y2=train_retention_hdi[mask, :][:, 1],\n",
    "        alpha=0.3,\n",
    "        color=\"C0\",\n",
    "        label=\"94% HDI (train)\",\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=train_period[train_period_idx[mask]],\n",
    "        y=train_retention[mask],\n",
    "        color=\"C0\",\n",
    "        marker=\"o\",\n",
    "        label=\"observed retention (train)\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    cohort_name = (\n",
    "        pd.to_datetime(train_cohort_encoder.classes_[cohort_index]).date().isoformat()\n",
    "    )\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set(title=f\"Retention HDI - Cohort {cohort_name}\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "cohort_index_to_plot = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=np.ceil(len(cohort_index_to_plot) / 2).astype(int),\n",
    "    ncols=2,\n",
    "    figsize=(15, 10),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for cohort_index, ax in zip(cohort_index_to_plot, axes.flatten()):\n",
    "    plot_train_retention_hdi_cohort(cohort_index=cohort_index, ax=ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the linear model case, we are capturing the retention rate development over time. The uncertainty estimates are also quite similar to the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_revenue_hdi = az.hdi(ary=posterior_predictive.posterior_predictive)[\"revenue_estimated\"]\n",
    "\n",
    "\n",
    "def plot_train_revenue_hdi_cohort(cohort_index: int, ax: plt.Axes) -> plt.Axes:\n",
    "\n",
    "    mask = train_cohort_idx == cohort_index\n",
    "\n",
    "    ax.fill_between(\n",
    "        x=train_period[train_period_idx[mask]],\n",
    "        y1=train_revenue_hdi[mask, :][:, 0],\n",
    "        y2=train_revenue_hdi[mask, :][:, 1],\n",
    "        alpha=0.3,\n",
    "        color=\"C0\",\n",
    "        label=\"94% HDI (train)\",\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=train_period[train_period_idx[mask]],\n",
    "        y=train_revenue[mask],\n",
    "        color=\"C0\",\n",
    "        marker=\"o\",\n",
    "        label=\"observed revenue (train)\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    cohort_name = (\n",
    "        pd.to_datetime(train_cohort_encoder.classes_[cohort_index]).date().isoformat()\n",
    "    )\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set(title=f\"revenue HDI - Cohort {cohort_name}\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "cohort_index_to_plot = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=np.ceil(len(cohort_index_to_plot) / 2).astype(int),\n",
    "    ncols=2,\n",
    "    figsize=(15, 10),\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for cohort_index, ax in zip(cohort_index_to_plot, axes.flatten()):\n",
    "    plot_train_revenue_hdi_cohort(cohort_index=cohort_index, ax=ax)\n",
    "\n",
    "fig.suptitle(\"Revenue Predictions\", y=1.03, fontsize=16);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Now we transform the test data to the same format as the training data and use the model to predict the retention rates. Note that we are using the scalers and encoders from the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_red_df = test_data_df.query(\"cohort_age > 0\")\n",
    "test_data_red_df = test_data_red_df[\n",
    "    test_data_red_df[\"cohort\"].isin(train_data_red_df[\"cohort\"].unique())\n",
    "].reset_index(drop=True)\n",
    "test_obs_idx = test_data_red_df.index.to_numpy()\n",
    "test_n_users = test_data_red_df[\"n_users\"].to_numpy()\n",
    "test_n_active_users = test_data_red_df[\"n_active_users\"].to_numpy()\n",
    "test_retention = test_data_red_df[\"retention\"].to_numpy()\n",
    "test_revenue = test_data_red_df[\"revenue\"].to_numpy()\n",
    "\n",
    "test_cohort = test_data_red_df[\"cohort\"].to_numpy()\n",
    "test_cohort_idx = train_cohort_encoder.transform(test_cohort).flatten()\n",
    "\n",
    "test_data_red_df[\"month\"] = test_data_red_df[\"period\"].dt.strftime(\"%m\").astype(int)\n",
    "test_data_red_df[\"cohort_month\"] = test_data_red_df[\"cohort\"].dt.strftime(\"%m\").astype(int)\n",
    "test_data_red_df[\"period_month\"] = test_data_red_df[\"period\"].dt.strftime(\"%m\").astype(int)\n",
    "x_test = test_data_red_df[features]\n",
    "\n",
    "test_age = test_data_red_df[\"age\"].to_numpy()\n",
    "test_age_scaled = train_age_scaler.transform(test_age.reshape(-1, 1)).flatten()\n",
    "test_cohort_age = test_data_red_df[\"cohort_age\"].to_numpy()\n",
    "test_cohort_age_scaled = train_cohort_age_scaler.transform(\n",
    "    test_cohort_age.reshape(-1, 1)\n",
    ").flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Sample Posterior Predictions\n",
    "\n",
    "Now we want to see out-of-sample predictions from this model. To begin, we need to compute the posterior predictive distribution for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    pm.set_data(\n",
    "        new_data={\n",
    "            \"age_scaled\": test_age_scaled,\n",
    "            \"cohort_age_scaled\": test_cohort_age_scaled,\n",
    "            \"x\": x_test,\n",
    "            \"n_users\": test_n_users,\n",
    "            \"n_active_users\": np.ones_like(\n",
    "                test_n_active_users\n",
    "            ),  # Dummy data to make coords work! We are not using this at prediction time!\n",
    "            \"revenue\": np.ones_like(\n",
    "                test_revenue\n",
    "            ),  # Dummy data to make coords work! We are not using this at prediction time!\n",
    "        },\n",
    "        coords={\"obs\": test_obs_idx},\n",
    "    )\n",
    "    idata.extend(\n",
    "        pm.sample_posterior_predictive(\n",
    "            trace=idata,\n",
    "            var_names=[\n",
    "                \"p\",\n",
    "                \"mu\",\n",
    "                \"n_active_users_estimated\",\n",
    "                \"revenue_estimated\",\n",
    "                \"mean_revenue_per_user\",\n",
    "                \"mean_revenue_per_active_user\",\n",
    "            ],\n",
    "            idata_kwargs={\"coords\": {\"obs\": test_obs_idx}},\n",
    "            random_seed=rng,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention Rate Out-of-Sample Predictions\n",
    "\n",
    "Finally we compute the posterior retention rate distributions for the test data and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_posterior_retention = (\n",
    "    idata.posterior_predictive[\"n_active_users_estimated\"] / test_n_users[np.newaxis, None]\n",
    ")\n",
    "\n",
    "test_retention_hdi = az.hdi(ary=test_posterior_retention)[\"n_active_users_estimated\"]\n",
    "test_revenue_hdi = az.hdi(ary=idata.posterior_predictive)[\"revenue_estimated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_retention_hdi_cohort(cohort_index: int, ax: plt.Axes) -> plt.Axes:\n",
    "    mask = test_cohort_idx == cohort_index\n",
    "\n",
    "    test_period_range = test_data_red_df.query(\n",
    "        f\"cohort == '{train_cohort_encoder.classes_[cohort_index]}'\"\n",
    "    )[\"period\"]\n",
    "\n",
    "    ax.fill_between(\n",
    "        x=test_period_range,\n",
    "        y1=test_retention_hdi[mask, :][:, 0],\n",
    "        y2=test_retention_hdi[mask, :][:, 1],\n",
    "        alpha=0.3,\n",
    "        color=\"C1\",\n",
    "        label=\"94% HDI (test)\",\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=test_period_range,\n",
    "        y=test_retention[mask],\n",
    "        color=\"C1\",\n",
    "        marker=\"o\",\n",
    "        label=\"observed retention (test)\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_index_to_plot = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(cohort_index_to_plot),\n",
    "    ncols=1,\n",
    "    figsize=(12, 15),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for cohort_index, ax in zip(cohort_index_to_plot, axes.flatten()):\n",
    "    plot_train_retention_hdi_cohort(cohort_index=cohort_index, ax=ax)\n",
    "    plot_test_retention_hdi_cohort(cohort_index=cohort_index, ax=ax)\n",
    "    ax.axvline(\n",
    "        x=pd.to_datetime(period_train_test_split),\n",
    "        color=\"black\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"train/test split\",\n",
    "    )\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "fig.suptitle(\"Retention Predictions\", y=1.03, fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_revenue_hdi_cohort(cohort_index: int, ax: plt.Axes) -> plt.Axes:\n",
    "    mask = test_cohort_idx == cohort_index\n",
    "\n",
    "    test_period_range = test_data_red_df.query(\n",
    "        f\"cohort == '{train_cohort_encoder.classes_[cohort_index]}'\"\n",
    "    )[\"period\"]\n",
    "\n",
    "    ax.fill_between(\n",
    "        x=test_period_range,\n",
    "        y1=test_revenue_hdi[mask, :][:, 0],\n",
    "        y2=test_revenue_hdi[mask, :][:, 1],\n",
    "        alpha=0.3,\n",
    "        color=\"C1\",\n",
    "        label=\"94% HDI (test)\",\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=test_period_range,\n",
    "        y=test_revenue[mask],\n",
    "        color=\"C1\",\n",
    "        marker=\"o\",\n",
    "        label=\"observed revenue (test)\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_index_to_plot = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(cohort_index_to_plot),\n",
    "    ncols=1,\n",
    "    figsize=(12, 15),\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for cohort_index, ax in zip(cohort_index_to_plot, axes.flatten()):\n",
    "    plot_train_revenue_hdi_cohort(cohort_index=cohort_index, ax=ax)\n",
    "    plot_test_revenue_hdi_cohort(cohort_index=cohort_index, ax=ax)\n",
    "    ax.axvline(\n",
    "        x=pd.to_datetime(period_train_test_split),\n",
    "        color=\"black\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"train/pred split\",\n",
    "    )\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "fig.suptitle(\"revenue Predictions\", y=1.03, fontsize=16);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "website_projects-1IZj_WTw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "867ba48c05011db76db56a12fb95ccd32f7ac276df8f4ae698e0d475911a6ba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
