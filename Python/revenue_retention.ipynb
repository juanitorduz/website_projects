{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort Revenue Retention Analysis: A Bayesian Approach\n",
    "\n",
    "In this notebook we extend the cohort retention model presented in the post [Cohort Retention Analysis with BART](https://juanitorduz.github.io/retention_bart/) so that we just model retention **and**  per cohort simultaneously (we recommend to read the referenced post before this one).  The idea is to keep modeling the retention using a Bayesian Additive Regression Tree (BART) model (see [`pymc-bart`](https://www.pymc.io/projects/bart/en/latest/)) and model the revenue per cohort linearly using a Gamma distribution. We couple the retention and revenue components in a similar way as presented in notebook [Introduction to Bayesian A/B Testing](https://www.pymc.io/projects/examples/en/latest/case_studies/bayesian_ab_testing_introduction.html). For this simulated example we use synthetic data set, see the blog post [A Simple Cohort Retention Analysis in PyMC](https://juanitorduz.github.io/retention/) For more details. [Here](https://github.com/juanitorduz/website_projects/blob/master/data/retention_data.csv) you can find the data to reproduce the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pymc_bart as pmb\n",
    "import pytensor.tensor as pt\n",
    "import seaborn as sns\n",
    "from scipy.special import logit\n",
    "from sklearn.preprocessing import MaxAbsScaler, LabelEncoder\n",
    "\n",
    "\n",
    "plt.style.use(\"bmh\")\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 7]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = sum(map(ord, \"retention\"))\n",
    "rng: np.random.Generator = np.random.default_rng(seed=seed)\n",
    "random_seed_int: int = rng.integers(low=0, high=100, size=1).item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "We start by reading the data from previous posts (see [here](https://github.com/juanitorduz/website_projects/blob/master/Python/retantion_data.py) for the code to generate the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"../data/retention_data.csv\", parse_dates=[\"cohort\", \"period\"])\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new component that we have is `revenue` which represents the revenue per cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"revenue\"].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In order to understand the user vs revenue relation, let's compute the revenue per users and per *active* users. The former represent the overall cohort contribution and the latter the contribution of the active users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"revenue_per_users\"] = data_df[\"revenue\"] / data_df[\"n_users\"]\n",
    "data_df[\"revenue_per_active_users\"] = data_df[\"revenue\"] / data_df[\"n_active_users\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we have certain `periods` where we do not have active users. This induces `NaN` values in the `revenue_per_active_users`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.query(\"revenue_per_active_users.isna()\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill these missing values with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.fillna(value={\"revenue_per_active_users\": 0.0}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a data train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_train_test_split = \"2022-11-01\"\n",
    "\n",
    "train_data_df = data_df.query(\"period <= @period_train_test_split\")\n",
    "test_data_df = data_df.query(\"period > @period_train_test_split\")\n",
    "test_data_df = test_data_df[\n",
    "    test_data_df[\"cohort\"].isin(train_data_df[\"cohort\"].unique())\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "For a detailed EDA of the data, please refer to the previous posts ([A Simple Cohort Retention Analysis in PyMC](https://juanitorduz.github.io/retention/) and [Cohort Retention Analysis with BART](https://juanitorduz.github.io/retention_bart/)). Here we want to focus in the retention and revenue relation.  First, let's recall how the retention matrix looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "fmt = lambda y, _: f\"{y :0.0%}\"\n",
    "\n",
    "(\n",
    "    train_data_df.assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\"),\n",
    "        period=lambda df: df[\"period\"].dt.strftime(\"%Y-%m\"),\n",
    "    )\n",
    "    .query(\"cohort_age != 0\")\n",
    "    .filter([\"cohort\", \"period\", \"retention\"])\n",
    "    .pivot(index=\"cohort\", columns=\"period\", values=\"retention\")\n",
    "    .pipe(\n",
    "        (sns.heatmap, \"data\"),\n",
    "        cmap=\"viridis_r\",\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"black\",\n",
    "        annot=True,\n",
    "        fmt=\"0.0%\",\n",
    "        cbar_kws={\"format\": mtick.FuncFormatter(fmt)},\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_title(\"Retention by Cohort and Period\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key observation is that the retention matrix has a clear seasonality pattern (in the `period`, i.e. observation variable) and seems to be changing as a function of the `cohort` (i.e. the group variable). This motivates using is a  BART model to capture complex interactions between the `period`, `cohort` and seasonal variables. In the next figure we plot the  retention rate by cohort over time (period) to future illustrate the seasonality pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "sns.lineplot(\n",
    "    x=\"period\",\n",
    "    y=\"retention\",\n",
    "    hue=\"cohort\",\n",
    "    palette=\"viridis_r\",\n",
    "    alpha=0.8,\n",
    "    data=train_data_df.query(\"cohort_age > 0\").assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\")\n",
    "    ),\n",
    "    ax=ax,\n",
    ")\n",
    "ax.legend(title=\"cohort\", loc=\"center left\", bbox_to_anchor=(1, 0.5), fontsize=7.5)\n",
    "ax.set(title=\"Retention by Cohort and Period\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the retention rate by itself hides how *big* is the cohort. At the very end, one os not just interested in the retention rate but in the value of the cohort. We can start by looking into absolute number of active users per cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "fmt = lambda y, _: f\"{y :0.0f}\"\n",
    "\n",
    "(\n",
    "    train_data_df.assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\"),\n",
    "        period=lambda df: df[\"period\"].dt.strftime(\"%Y-%m\"),\n",
    "    )\n",
    "    .query(\"cohort_age != 0\")\n",
    "    .filter([\"cohort\", \"period\", \"n_active_users\"])\n",
    "    .pivot(index=\"cohort\", columns=\"period\", values=\"n_active_users\")\n",
    "    .pipe(\n",
    "        (sns.heatmap, \"data\"),\n",
    "        cmap=\"viridis_r\",\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"black\",\n",
    "        annot=True,\n",
    "        annot_kws={\"fontsize\":8},\n",
    "        fmt=\"0.0f\",\n",
    "        cbar_kws={\"format\": mtick.FuncFormatter(fmt)},\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_title(\"Active Users by Cohort and Period\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The younger cohorts are much smaller than the older cohorts. Next, we plot the revenue absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "fmt = lambda y, _: f\"{y :0.0f}\"\n",
    "\n",
    "(\n",
    "    train_data_df.assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\"),\n",
    "        period=lambda df: df[\"period\"].dt.strftime(\"%Y-%m\"),\n",
    "    )\n",
    "    .query(\"cohort_age != 0\")\n",
    "    .filter([\"cohort\", \"period\", \"revenue\"])\n",
    "    .pivot(index=\"cohort\", columns=\"period\", values=\"revenue\")\n",
    "    .pipe(\n",
    "        (sns.heatmap, \"data\"),\n",
    "        cmap=\"viridis_r\",\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"black\",\n",
    "        annot=True,\n",
    "        annot_kws={\"fontsize\": 6},\n",
    "        fmt=\"0.0f\",\n",
    "        cbar_kws={\"format\": mtick.FuncFormatter(fmt)},\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_title(\"Revenue by Cohort and Period\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern looks very similar as the number of active users. Hence, we expect the quotient `revenue_per_active_users` to be relatively stable across cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "fmt = lambda y, _: f\"{y :0.0f}\"\n",
    "\n",
    "(\n",
    "    train_data_df.assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\"),\n",
    "        period=lambda df: df[\"period\"].dt.strftime(\"%Y-%m\"),\n",
    "    )\n",
    "    .query(\"cohort_age != 0\")\n",
    "    .filter([\"cohort\", \"period\", \"revenue_per_active_users\"])\n",
    "    .pivot(index=\"cohort\", columns=\"period\", values=\"revenue_per_active_users\")\n",
    "    .pipe(\n",
    "        (sns.heatmap, \"data\"),\n",
    "        cmap=\"viridis_r\",\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"black\",\n",
    "        annot=True,\n",
    "        annot_kws={\"fontsize\": 9},\n",
    "        fmt=\"0.0f\",\n",
    "        cbar_kws={\"format\": mtick.FuncFormatter(fmt)},\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_title(\"Revenue by Cohort and Period\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that this ratio does not show a strong seasonality pattern. This suggest that for revenue the model we can simply add the seasonality pattern into the retention rate component. In addition, note that the `revenue_per_active_users` seems to decrease with the `cohort_age` linearly. In a similar manner, it seems to increase with the `age` of the cohort linearly as well.\n",
    "\n",
    "Finally, we plot the `revenue_per_users` which includes also users which are not active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "fmt = lambda y, _: f\"{y :0.0f}\"\n",
    "\n",
    "(\n",
    "    train_data_df.assign(\n",
    "        cohort=lambda df: df[\"cohort\"].dt.strftime(\"%Y-%m\"),\n",
    "        period=lambda df: df[\"period\"].dt.strftime(\"%Y-%m\"),\n",
    "    )\n",
    "    .query(\"cohort_age != 0\")\n",
    "    .filter([\"cohort\", \"period\", \"revenue_per_users\"])\n",
    "    .pivot(index=\"cohort\", columns=\"period\", values=\"revenue_per_users\")\n",
    "    .pipe(\n",
    "        (sns.heatmap, \"data\"),\n",
    "        cmap=\"viridis_r\",\n",
    "        linewidths=0.2,\n",
    "        linecolor=\"black\",\n",
    "        annot=True,\n",
    "        annot_kws={\"fontsize\": 9},\n",
    "        fmt=\"0.0f\",\n",
    "        cbar_kws={\"format\": mtick.FuncFormatter(fmt)},\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_title(\"Revenue by Cohort and Period\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is no surprise that we observe the seasonal component again (as the cohort size is fixed)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Motivates by the analysis above we suggest the following retention-revenue model.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Revenue} & \\sim \\text{Gamma}(N_{\\text{active}}, \\lambda) \\\\\n",
    "\\log(\\lambda) = (& \\text{intercept} \\\\\n",
    "    & + \\beta_{\\text{cohort age}} \\text{cohort age} \\\\\n",
    "    & + \\beta_{\\text{age}} \\text{age} \\\\\n",
    "    & + \\beta_{\\text{cohort age} \\times \\text{age}} \\text{cohort age} \\times \\text{age} ) \\\\\n",
    "N_{\\text{active}} & \\sim \\text{Binomial}(N_{\\text{total}}, p) \\\\\n",
    "\\textrm{logit}(p) & = \\text{BART}(\\text{cohort age}, \\text{age}, \\text{month})\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations\n",
    "\n",
    "We do similar transformations as in the previous posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(float).eps\n",
    "train_data_red_df = train_data_df.query(\"cohort_age > 0\").reset_index(drop=True)\n",
    "train_obs_idx = train_data_red_df.index.to_numpy()\n",
    "train_n_users = train_data_red_df[\"n_users\"].to_numpy()\n",
    "train_n_active_users = train_data_red_df[\"n_active_users\"].to_numpy()\n",
    "train_retention = train_data_red_df[\"retention\"].to_numpy()\n",
    "train_retention_logit = logit(train_retention + eps)\n",
    "train_data_red_df[\"month\"] = train_data_red_df[\"period\"].dt.strftime(\"%m\").astype(int)\n",
    "train_data_red_df[\"cohort_month\"] = (\n",
    "    train_data_red_df[\"cohort\"].dt.strftime(\"%m\").astype(int)\n",
    ")\n",
    "train_data_red_df[\"period_month\"] = (\n",
    "    train_data_red_df[\"period\"].dt.strftime(\"%m\").astype(int)\n",
    ")\n",
    "train_revenue = train_data_red_df[\"revenue\"].to_numpy() + eps\n",
    "train_revenue_per_user = train_revenue / (train_n_active_users + eps)\n",
    "\n",
    "train_cohort = train_data_red_df[\"cohort\"].to_numpy()\n",
    "train_cohort_encoder = LabelEncoder()\n",
    "train_cohort_idx = train_cohort_encoder.fit_transform(train_cohort).flatten()\n",
    "train_period = train_data_red_df[\"period\"].to_numpy()\n",
    "train_period_encoder = LabelEncoder()\n",
    "train_period_idx = train_period_encoder.fit_transform(train_period).flatten()\n",
    "\n",
    "features: list[str] = [\"age\", \"cohort_age\", \"month\"]\n",
    "x_train = train_data_red_df[features]\n",
    "\n",
    "train_age = train_data_red_df[\"age\"].to_numpy()\n",
    "train_age_scaler = MaxAbsScaler()\n",
    "train_age_scaled = train_age_scaler.fit_transform(train_age.reshape(-1, 1)).flatten()\n",
    "train_cohort_age = train_data_red_df[\"cohort_age\"].to_numpy()\n",
    "train_cohort_age_scaler = MaxAbsScaler()\n",
    "train_cohort_age_scaled = train_cohort_age_scaler.fit_transform(\n",
    "    train_cohort_age.reshape(-1, 1)\n",
    ").flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification\n",
    "\n",
    "Now we are ready to specify the model in PyMC.\n",
    "- For the retention component please see the details presented in the post [Cohort Retention Analysis with BART](https://juanitorduz.github.io/retention_bart/).\n",
    "- The retention-revenue coupling is motivates by the model presented in the example notebook the post [Introduction to Bayesian A/B Testing](https://www.pymc.io/projects/examples/en/latest/case_studies/bayesian_ab_testing_introduction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"feature\": features}) as model:\n",
    "\n",
    "    # --- Data ---\n",
    "    model.add_coord(name=\"obs\", values=train_obs_idx, mutable=True)\n",
    "    age_scaled = pm.MutableData(name=\"age_scaled\", value=train_age_scaled, dims=\"obs\")\n",
    "    cohort_age_scaled = pm.MutableData(\n",
    "        name=\"cohort_age_scaled\", value=train_cohort_age_scaled, dims=\"obs\"\n",
    "    )\n",
    "    x = pm.MutableData(name=\"x\", value=x_train, dims=(\"obs\", \"feature\"))\n",
    "    n_users = pm.MutableData(name=\"n_users\", value=train_n_users, dims=\"obs\")\n",
    "    n_active_users = pm.MutableData(\n",
    "        name=\"n_active_users\", value=train_n_active_users, dims=\"obs\"\n",
    "    )\n",
    "    revenue = pm.MutableData(name=\"revenue\", value=train_revenue, dims=\"obs\")\n",
    "\n",
    "    # --- Priors ---\n",
    "    intercept = pm.Normal(name=\"intercept\", mu=0, sigma=1)\n",
    "    b_age_scaled = pm.Normal(name=\"b_age_scaled\", mu=0, sigma=1)\n",
    "    b_cohort_age_scaled = pm.Normal(name=\"b_cohort_age_scaled\", mu=0, sigma=1)\n",
    "    b_age_cohort_age_interaction = pm.Normal(\n",
    "        name=\"b_age_cohort_age_interaction\", mu=0, sigma=1\n",
    "    )\n",
    "\n",
    "    # --- Parametrization ---\n",
    "    # The BART component models the image of the retention rate under the\n",
    "    # logit transform so that the range is not constrained to [0, 1].\n",
    "    mu = pmb.BART(name=\"mu\", X=x, Y=train_retention_logit, m=50, dims=\"obs\")\n",
    "    # We use the inverse logit transform to get the retention rate back into [0, 1].\n",
    "    p = pm.Deterministic(name=\"p\", var=pm.math.invlogit(mu), dims=\"obs\")\n",
    "    # We add a small epsilon to avoid numerical issues.\n",
    "    p = pt.switch(pt.eq(p, 0), eps, p)\n",
    "    p = pt.switch(pt.eq(p, 1), 1 - eps, p)\n",
    "\n",
    "    # For the revenue component we use a Gamma distribution where we combine the number\n",
    "    # of estimated active users with the average revenue per user.\n",
    "    lam_log = pm.Deterministic(\n",
    "        name=\"lam_log\",\n",
    "        var=intercept\n",
    "        + b_age_scaled * age_scaled\n",
    "        + b_cohort_age_scaled * cohort_age_scaled\n",
    "        + b_age_cohort_age_interaction * age_scaled * cohort_age_scaled,\n",
    "        dims=\"obs\",\n",
    "    )\n",
    "\n",
    "    lam = pm.Deterministic(name=\"lam\", var=pm.math.exp(lam_log), dims=\"obs\")\n",
    "\n",
    "    # --- Likelihood ---\n",
    "    n_active_users_estimated = pm.Binomial(\n",
    "        name=\"n_active_users_estimated\",\n",
    "        n=n_users,\n",
    "        p=p,\n",
    "        observed=n_active_users,\n",
    "        dims=\"obs\",\n",
    "    )\n",
    "\n",
    "    x = pm.Gamma(\n",
    "        name=\"revenue_estimated\",\n",
    "        alpha=n_active_users_estimated + eps,\n",
    "        beta=lam,\n",
    "        observed=revenue,\n",
    "        dims=\"obs\",\n",
    "    )\n",
    "\n",
    "    mean_revenue_per_user = pm.Deterministic(\n",
    "        name=\"mean_revenue_per_user\", var=(1 / lam), dims=\"obs\"\n",
    "    )\n",
    "    pm.Deterministic(\n",
    "        name=\"mean_revenue_per_active_user\", var=p * mean_revenue_per_user, dims=\"obs\"\n",
    "    )\n",
    "\n",
    "pm.model_to_graphviz(model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "Now we are ready to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata = pm.sample(draws=2_000, chains=4, random_seed=rng)\n",
    "    posterior_predictive = pm.sample_posterior_predictive(trace=idata, random_seed=rng)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Diagnostics\n",
    "\n",
    "We look into the posterior predictive check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_ppc(\n",
    "    data=posterior_predictive,\n",
    "    kind=\"cumulative\",\n",
    "    observed_rug=True,\n",
    "    random_seed=random_seed_int,\n",
    ")\n",
    "ax[0].set(\n",
    "    title=\"Posterior Predictive Check (Retention)\",\n",
    "    xscale=\"log\",\n",
    "    xlabel=\"likelihood (n_active_users) - log scale\",\n",
    ")\n",
    "ax[1].set(\n",
    "    title=\"Posterior Predictive Check (Revenue)\",\n",
    "    xscale=\"log\",\n",
    "    xlabel=\"likelihood (revenue) - log scale\",\n",
    "    xlim=(1, None), # to avoid plotting the clipped value `eps`.\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata.sample_stats[\"diverging\"].sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = az.plot_trace(\n",
    "    data=idata,\n",
    "    var_names=[\n",
    "        \"intercept\",\n",
    "        \"b_age_scaled\",\n",
    "        \"b_cohort_age_scaled\",\n",
    "        \"b_age_cohort_age_interaction\",\n",
    "    ],\n",
    "    compact=True,\n",
    "    kind=\"rank_bars\",\n",
    "    backend_kwargs={\"figsize\": (12, 10), \"layout\": \"constrained\"},\n",
    ")\n",
    "plt.gcf().suptitle(\"Model Trace\", fontsize=16);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be doing a good job ðŸ™‚ ! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention Rate In-Sample Predictions\n",
    "\n",
    "Let's see how the model performs in-sample. We plot the retention rate posterior mean predictions for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posterior_retention = (\n",
    "    posterior_predictive.posterior_predictive[\"n_active_users_estimated\"]\n",
    "    / train_n_users[np.newaxis, None]\n",
    ")\n",
    "train_posterior_retention_mean = az.extract(\n",
    "    data=train_posterior_retention, var_names=[\"n_active_users_estimated\"]\n",
    ").mean(\"sample\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "sns.scatterplot(\n",
    "    x=\"retention\",\n",
    "    y=\"posterior_retention_mean\",\n",
    "    data=train_data_red_df.assign(\n",
    "        posterior_retention_mean=train_posterior_retention_mean\n",
    "    ),\n",
    "    hue=\"age\",\n",
    "    palette=\"viridis_r\",\n",
    "    size=\"n_users\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.axline(xy1=(0.3, 0.3), slope=1, color=\"black\", linestyle=\"--\", label=\"diagonal\")\n",
    "ax.legend()\n",
    "ax.set(title=\"Posterior Predictive - Retention Mean\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posterior_revenue_mean = az.extract(\n",
    "    data=posterior_predictive,\n",
    "    group=\"posterior_predictive\",\n",
    "    var_names=[\"revenue_estimated\"],\n",
    ").mean(\"sample\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "sns.scatterplot(\n",
    "    x=\"revenue\",\n",
    "    y=\"posterior_revenue_mean\",\n",
    "    data=train_data_red_df.assign(posterior_revenue_mean=train_posterior_revenue_mean),\n",
    "    hue=\"age\",\n",
    "    palette=\"viridis_r\",\n",
    "    size=\"n_users\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.axline(xy1=(1e5, 1e5), slope=1, color=\"black\", linestyle=\"--\", label=\"diagonal\")\n",
    "ax.legend()\n",
    "ax.set(\n",
    "    title=\"Posterior Predictive - Revenue Mean\",\n",
    "    xscale=\"log\",\n",
    "    yscale=\"log\",\n",
    "    xlabel=\"revenue (log)\",\n",
    "    ylabel=\"posterior_revenue_mean (log)\",\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look into the uncertainty estimates for a subset of individual cohorts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retention_hdi = az.hdi(ary=train_posterior_retention)[\"n_active_users_estimated\"]\n",
    "\n",
    "\n",
    "def plot_train_retention_hdi_cohort(cohort_index: int, ax: plt.Axes) -> plt.Axes:\n",
    "\n",
    "    mask = train_cohort_idx == cohort_index\n",
    "\n",
    "    ax.fill_between(\n",
    "        x=train_period[train_period_idx[mask]],\n",
    "        y1=train_retention_hdi[mask, :][:, 0],\n",
    "        y2=train_retention_hdi[mask, :][:, 1],\n",
    "        alpha=0.3,\n",
    "        color=\"C0\",\n",
    "        label=\"94% HDI (train)\",\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=train_period[train_period_idx[mask]],\n",
    "        y=train_retention[mask],\n",
    "        color=\"C0\",\n",
    "        marker=\"o\",\n",
    "        label=\"observed retention (train)\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    cohort_name = (\n",
    "        pd.to_datetime(train_cohort_encoder.classes_[cohort_index]).date().isoformat()\n",
    "    )\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set(title=f\"Retention HDI - Cohort {cohort_name}\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "cohort_index_to_plot = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=np.ceil(len(cohort_index_to_plot) / 2).astype(int),\n",
    "    ncols=2,\n",
    "    figsize=(15, 10),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for cohort_index, ax in zip(cohort_index_to_plot, axes.flatten()):\n",
    "    plot_train_retention_hdi_cohort(cohort_index=cohort_index, ax=ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the linear model case, we are capturing the retention rate development over time. The uncertainty estimates are also quite similar to the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_revenue_hdi = az.hdi(ary=posterior_predictive.posterior_predictive)[\"revenue_estimated\"]\n",
    "\n",
    "\n",
    "def plot_train_revenue_hdi_cohort(cohort_index: int, ax: plt.Axes) -> plt.Axes:\n",
    "\n",
    "    mask = train_cohort_idx == cohort_index\n",
    "\n",
    "    ax.fill_between(\n",
    "        x=train_period[train_period_idx[mask]],\n",
    "        y1=train_revenue_hdi[mask, :][:, 0],\n",
    "        y2=train_revenue_hdi[mask, :][:, 1],\n",
    "        alpha=0.3,\n",
    "        color=\"C0\",\n",
    "        label=\"94% HDI (train)\",\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=train_period[train_period_idx[mask]],\n",
    "        y=train_revenue[mask],\n",
    "        color=\"C0\",\n",
    "        marker=\"o\",\n",
    "        label=\"observed revenue (train)\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    cohort_name = (\n",
    "        pd.to_datetime(train_cohort_encoder.classes_[cohort_index]).date().isoformat()\n",
    "    )\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set(title=f\"revenue HDI - Cohort {cohort_name}\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "cohort_index_to_plot = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=np.ceil(len(cohort_index_to_plot) / 2).astype(int),\n",
    "    ncols=2,\n",
    "    figsize=(15, 10),\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for cohort_index, ax in zip(cohort_index_to_plot, axes.flatten()):\n",
    "    plot_train_revenue_hdi_cohort(cohort_index=cohort_index, ax=ax)\n",
    "\n",
    "fig.suptitle(\"Revenue Predictions\", y=1.03, fontsize=16);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Now we transform the test data to the same format as the training data and use the model to predict the retention rates. Note that we are using the scalers and encoders from the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_red_df = test_data_df.query(\"cohort_age > 0\")\n",
    "test_data_red_df = test_data_red_df[\n",
    "    test_data_red_df[\"cohort\"].isin(train_data_red_df[\"cohort\"].unique())\n",
    "].reset_index(drop=True)\n",
    "test_obs_idx = test_data_red_df.index.to_numpy()\n",
    "test_n_users = test_data_red_df[\"n_users\"].to_numpy()\n",
    "test_n_active_users = test_data_red_df[\"n_active_users\"].to_numpy()\n",
    "test_retention = test_data_red_df[\"retention\"].to_numpy()\n",
    "test_revenue = test_data_red_df[\"revenue\"].to_numpy()\n",
    "\n",
    "test_cohort = test_data_red_df[\"cohort\"].to_numpy()\n",
    "test_cohort_idx = train_cohort_encoder.transform(test_cohort).flatten()\n",
    "\n",
    "test_data_red_df[\"month\"] = test_data_red_df[\"period\"].dt.strftime(\"%m\").astype(int)\n",
    "test_data_red_df[\"cohort_month\"] = test_data_red_df[\"cohort\"].dt.strftime(\"%m\").astype(int)\n",
    "test_data_red_df[\"period_month\"] = test_data_red_df[\"period\"].dt.strftime(\"%m\").astype(int)\n",
    "x_test = test_data_red_df[features]\n",
    "\n",
    "test_age = test_data_red_df[\"age\"].to_numpy()\n",
    "test_age_scaled = train_age_scaler.transform(test_age.reshape(-1, 1)).flatten()\n",
    "test_cohort_age = test_data_red_df[\"cohort_age\"].to_numpy()\n",
    "test_cohort_age_scaled = train_cohort_age_scaler.transform(\n",
    "    test_cohort_age.reshape(-1, 1)\n",
    ").flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Sample Posterior Predictions\n",
    "\n",
    "Now we want to see out-of-sample predictions from this model. To begin, we need to compute the posterior predictive distribution for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    pm.set_data(\n",
    "        new_data={\n",
    "            \"age_scaled\": test_age_scaled,\n",
    "            \"cohort_age_scaled\": test_cohort_age_scaled,\n",
    "            \"x\": x_test,\n",
    "            \"n_users\": test_n_users,\n",
    "            \"n_active_users\": np.ones_like(\n",
    "                test_n_active_users\n",
    "            ),  # Dummy data to make coords work! We are not using this at prediction time!\n",
    "            \"revenue\": np.ones_like(\n",
    "                test_revenue\n",
    "            ),  # Dummy data to make coords work! We are not using this at prediction time!\n",
    "        },\n",
    "        coords={\"obs\": test_obs_idx},\n",
    "    )\n",
    "    idata.extend(\n",
    "        pm.sample_posterior_predictive(\n",
    "            trace=idata,\n",
    "            var_names=[\n",
    "                \"p\",\n",
    "                \"mu\",\n",
    "                \"n_active_users_estimated\",\n",
    "                \"revenue_estimated\",\n",
    "                \"mean_revenue_per_user\",\n",
    "                \"mean_revenue_per_active_user\",\n",
    "            ],\n",
    "            idata_kwargs={\"coords\": {\"obs\": test_obs_idx}},\n",
    "            random_seed=rng,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention Rate Out-of-Sample Predictions\n",
    "\n",
    "Finally we compute the posterior retention rate distributions for the test data and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_posterior_retention = (\n",
    "    idata.posterior_predictive[\"n_active_users_estimated\"] / test_n_users[np.newaxis, None]\n",
    ")\n",
    "\n",
    "test_retention_hdi = az.hdi(ary=test_posterior_retention)[\"n_active_users_estimated\"]\n",
    "test_revenue_hdi = az.hdi(ary=idata.posterior_predictive)[\"revenue_estimated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_retention_hdi_cohort(cohort_index: int, ax: plt.Axes) -> plt.Axes:\n",
    "    mask = test_cohort_idx == cohort_index\n",
    "\n",
    "    test_period_range = test_data_red_df.query(\n",
    "        f\"cohort == '{train_cohort_encoder.classes_[cohort_index]}'\"\n",
    "    )[\"period\"]\n",
    "\n",
    "    ax.fill_between(\n",
    "        x=test_period_range,\n",
    "        y1=test_retention_hdi[mask, :][:, 0],\n",
    "        y2=test_retention_hdi[mask, :][:, 1],\n",
    "        alpha=0.3,\n",
    "        color=\"C1\",\n",
    "        label=\"94% HDI (test)\",\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=test_period_range,\n",
    "        y=test_retention[mask],\n",
    "        color=\"C1\",\n",
    "        marker=\"o\",\n",
    "        label=\"observed retention (test)\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_index_to_plot = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(cohort_index_to_plot),\n",
    "    ncols=1,\n",
    "    figsize=(12, 15),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for cohort_index, ax in zip(cohort_index_to_plot, axes.flatten()):\n",
    "    plot_train_retention_hdi_cohort(cohort_index=cohort_index, ax=ax)\n",
    "    plot_test_retention_hdi_cohort(cohort_index=cohort_index, ax=ax)\n",
    "    ax.axvline(\n",
    "        x=pd.to_datetime(period_train_test_split),\n",
    "        color=\"black\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"train/test split\",\n",
    "    )\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "fig.suptitle(\"Retention Predictions\", y=1.03, fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_revenue_hdi_cohort(cohort_index: int, ax: plt.Axes) -> plt.Axes:\n",
    "    mask = test_cohort_idx == cohort_index\n",
    "\n",
    "    test_period_range = test_data_red_df.query(\n",
    "        f\"cohort == '{train_cohort_encoder.classes_[cohort_index]}'\"\n",
    "    )[\"period\"]\n",
    "\n",
    "    ax.fill_between(\n",
    "        x=test_period_range,\n",
    "        y1=test_revenue_hdi[mask, :][:, 0],\n",
    "        y2=test_revenue_hdi[mask, :][:, 1],\n",
    "        alpha=0.3,\n",
    "        color=\"C1\",\n",
    "        label=\"94% HDI (test)\",\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=test_period_range,\n",
    "        y=test_revenue[mask],\n",
    "        color=\"C1\",\n",
    "        marker=\"o\",\n",
    "        label=\"observed revenue (test)\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_index_to_plot = [0, 1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(cohort_index_to_plot),\n",
    "    ncols=1,\n",
    "    figsize=(12, 15),\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for cohort_index, ax in zip(cohort_index_to_plot, axes.flatten()):\n",
    "    plot_train_revenue_hdi_cohort(cohort_index=cohort_index, ax=ax)\n",
    "    plot_test_revenue_hdi_cohort(cohort_index=cohort_index, ax=ax)\n",
    "    ax.axvline(\n",
    "        x=pd.to_datetime(period_train_test_split),\n",
    "        color=\"black\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"train/pred split\",\n",
    "    )\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "fig.suptitle(\"revenue Predictions\", y=1.03, fontsize=16);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "website_projects-1IZj_WTw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "867ba48c05011db76db56a12fb95ccd32f7ac276df8f4ae698e0d475911a6ba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
