{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd0c3dc",
   "metadata": {},
   "source": [
    "# Machine Learning for Optimization: Toy Example\n",
    "\n",
    "Based on my experience, whenever someone asks for a prediction (or forecasting) model, they actually do not need a prediction model per se. They typically want to answer causal questions or do some kind of optimization. I have covered some case studies on causal questions in previous posts (for example, [Introduction to Causal Inference with PPLs](https://juanitorduz.github.io/intro_causal_inference_ppl_pymc/) and [\"Using Data Science for Bad Decision-Making: A Case Study\"](https://juanitorduz.github.io/causal_inference_example/)). In this blog post, I want to focus on optimization. I found a little nice use case when working on adtech, where one is interested in optimizing bids to maximize the revenue (or any other target, like ROAS or lifetime value). How to set the bids? This is a huge active research area so this is by no means an exhaustive treatment. I want to focus on a small component on a recent paper: [\"Lightweight Auto-bidding based on Traffic Prediction in Live Advertising\"](https://arxiv.org/pdf/2508.06069) where the authors propose a method to set the bids by optimizing on the output of a fitted forecast model. I won't go into the paper scope, but rather focus on a self contained problem: *Algorithm 1 Algorithm BiCB*. The basic idea is as follows: In order to set bids on time intervals we can fit a forecasting model to predict the cumulative cost over the day based on time features and the current bid value $\\text{bid}_t$. To set the next bid $\\text{bid}_{t + 1}$ we can compare the forecast against the desired target (say, the expected cumulative daily budget at $t + 1$). We can adjust the under/over pacing by minimizing this difference. In other words, we want to steer achieving the target using the bids values through a *time machine* (i.e. a forecasting model) to generate counterfactuals. The paper works out this in certain level of detail, but the concrete implementation is a bit open. So here we do it by plain *brute force* (why not?). The whole idea is not to solve this concrete algorithm but to experiment on how to use machine learning methods for optimization purposes.\n",
    "\n",
    "We do not work on this specific case but work on a generic simulated example as a first iteration to simply get intuition on the techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430190fd",
   "metadata": {},
   "source": [
    "## Prepare Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02dbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbaee13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "seed: int = sum(map(ord, \"seed\"))\n",
    "rng: np.random.Generator = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c6a6c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "We start by generating some synthetic data with two features and a target variable (the data generating process is semi-arbitrary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regression_data(\n",
    "    rng: np.random.Generator, n_samples: int\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    x1 = rng.uniform(0, 1, size=n_samples)\n",
    "    x2 = rng.uniform(0, 1, size=n_samples)\n",
    "    y = (\n",
    "        (x1 + x2 - 0.5) ** 2\n",
    "        - x1**3\n",
    "        - 0.5\n",
    "        + np.sin(2 * np.pi * x1 * x2**2)\n",
    "        + rng.normal(0, 0.1, size=n_samples)\n",
    "    )\n",
    "    return np.c_[x1, x2], y\n",
    "\n",
    "\n",
    "x, y = make_regression_data(rng=rng, n_samples=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf65be",
   "metadata": {},
   "source": [
    "Let's plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "norm = mcolors.Normalize(vmin=-np.max(np.abs(y)), vmax=np.max(np.abs(y)))\n",
    "sc = ax.scatter(x[:, 0], x[:, 1], c=y, cmap=\"coolwarm\", norm=norm, edgecolors=\"black\")\n",
    "cbar = plt.colorbar(sc, ax=ax, pad=0.02)\n",
    "cbar.set_label(\"y\", fontsize=14)\n",
    "ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\", title=\"Regression Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5a0bb",
   "metadata": {},
   "source": [
    "## Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441a5b3",
   "metadata": {},
   "source": [
    "Here is the problem we are trying to solve:\n",
    "\n",
    "Given a fixed value of $x_2$ (in the bidding example, these could be time features like hour of the day), we want to find the value of $x_1$ that minimizes the difference between the predicted value of a machine learning model (the *time machine*) and a target value (say, cumulative daily budget).\n",
    "\n",
    "Hence, we should:\n",
    "\n",
    "1. Fit a machine learning model to learn $y$ as a function of $x_1$ and $x_2$.\n",
    "2. For a given value of $x_2$, find the value of $x_1$ that minimizes the difference between the predicted value of the model and a target value.\n",
    "\n",
    "Let's do it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f11bf2b",
   "metadata": {},
   "source": [
    "## Fit ML Model\n",
    "\n",
    "For this regression problem, we will use a great default model: the [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html). Here we skip the hyperparameters tuning and cross-validation for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84404e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HistGradientBoostingRegressor(random_state=seed)\n",
    "\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de0666",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "\n",
    "We can now generate predictions for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ed6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    nrows=1, ncols=2, sharex=True, sharey=True, figsize=(14, 6), layout=\"constrained\"\n",
    ")\n",
    "\n",
    "sc_0 = ax[0].scatter(\n",
    "    x[:, 0], x[:, 1], c=y, cmap=\"coolwarm\", norm=norm, edgecolors=\"black\"\n",
    ")\n",
    "\n",
    "norm = mcolors.Normalize(vmin=-np.max(np.abs(y)), vmax=np.max(np.abs(y)))\n",
    "cbar = plt.colorbar(sc_0, ax=ax, pad=0.02)\n",
    "cbar.set_label(r\"$y$\", fontsize=14)\n",
    "\n",
    "\n",
    "ax[0].set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\", title=\"Regression Data\")\n",
    "\n",
    "sc_1 = ax[1].scatter(\n",
    "    x[:, 0], x[:, 1], c=y_pred, cmap=\"coolwarm\", norm=norm, edgecolors=\"black\"\n",
    ")\n",
    "ax[1].set(xlabel=r\"$x_1$\", ylabel=None, title=\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y, y_pred)\n",
    "print(f\"MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098223b1",
   "metadata": {},
   "source": [
    "Overall, the model looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee33c8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.scatter(y, y_pred)\n",
    "ax.axline(xy1=(0, 0), slope=1, color=\"black\", linestyle=\"--\")\n",
    "ax.set(xlabel=\"True\", ylabel=\"Predicted\", title=\"Predicted vs True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df268c5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Optimization Step\n",
    "\n",
    "Now that we have a model, we can use it for optimization.\n",
    "\n",
    "First, let us define the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb4ec7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OptimizationParameters:\n",
    "    x2_fixed: float  # Fixed value of x2\n",
    "    y_target: float  # Target value\n",
    "    bounds: tuple[float, float]  # Bounds of x1\n",
    "\n",
    "    @property\n",
    "    def get_initial_guess(self) -> float:\n",
    "        return (self.bounds[0] + self.bounds[1]) / 2\n",
    "\n",
    "\n",
    "optimization_parameters = OptimizationParameters(\n",
    "    x2_fixed=0.65, y_target=0.5, bounds=(0.2, 0.6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c5bc7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Now, we use the fitted model to generate predictions by freezing the value of $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce701085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_at_x2_fixed(x1: float, x2_fixed: float) -> float:\n",
    "    return model.predict(np.c_[x1, x2_fixed]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be17d1",
   "metadata": {},
   "source": [
    "Let's visualize the predictions on a grid of $x_1$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d56a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of x1 values\n",
    "x1_grid = np.linspace(x[:, 0].min(), x[:, 0].max(), 100)\n",
    "# Vectorize the model prediction function\n",
    "y_pred_grid = np.vectorize(model_predict_at_x2_fixed)(\n",
    "    x1_grid, optimization_parameters.x2_fixed\n",
    ")\n",
    "\n",
    "# Get the mask of the points that are close to the fixed x2 value\n",
    "# (just for visualization purposes)\n",
    "mask = np.abs(x[:, 1] - optimization_parameters.x2_fixed) < 0.1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.scatter(\n",
    "    x[:, 0][mask],\n",
    "    y[mask],\n",
    "    c=y[mask],\n",
    "    cmap=\"coolwarm\",\n",
    "    norm=norm,\n",
    "    edgecolors=\"black\",\n",
    "    label=rf\"data points close to $x_2 = {optimization_parameters.x2_fixed}$\",\n",
    ")\n",
    "ax.plot(\n",
    "    x1_grid,\n",
    "    y_pred_grid,\n",
    "    c=\"black\",\n",
    "    linewidth=3,\n",
    "    label=r\"predictions on a $x_1$-grid\",\n",
    ")\n",
    "ax.legend(loc=\"upper left\", fontsize=12)\n",
    "ax.set(xlabel=r\"$x_1$\", ylabel=r\"$y$\", title=r\"$x_1$ vs $y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8600508b",
   "metadata": {},
   "source": [
    "We can zoom in to the region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb093f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.scatter(\n",
    "    x[:, 0][mask],\n",
    "    y[mask],\n",
    "    c=y[mask],\n",
    "    cmap=\"coolwarm\",\n",
    "    norm=norm,\n",
    "    edgecolors=\"black\",\n",
    "    label=rf\"data points close to $x_2 = {optimization_parameters.x2_fixed}$\",\n",
    ")\n",
    "ax.plot(\n",
    "    x1_grid,\n",
    "    y_pred_grid,\n",
    "    c=\"black\",\n",
    "    linewidth=3,\n",
    "    label=r\"predictions on a $x_1$-grid\",\n",
    ")\n",
    "ax.axhline(\n",
    "    optimization_parameters.y_target,\n",
    "    color=\"C2\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=3,\n",
    "    alpha=0.7,\n",
    "    label=r\"target\",\n",
    ")\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set(xlabel=r\"$x_1$\", ylabel=r\"$y$\", title=r\"$x_1$ vs $y$\", xlim=(0.15, 0.65))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a30b3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Now we can use [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) to find the value of $x_1$ that minimizes the difference between the predicted value of the model and the target value.\n",
    "\n",
    "Observe that tree-based models have piecewise constant predictions, so gradient-based optimizers (default BFGS) won't work (because the gradient is zero almost everywhere). We should use a gradient-free method like Nelder-Mead or Powell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54686fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to minimize\n",
    "def function_to_minimize(x, x2_fixed, y_target):\n",
    "    # We want to minimize the difference between the predicted\n",
    "    # value and the target value.\n",
    "    return abs(model_predict_at_x2_fixed(x, x2_fixed) - y_target)\n",
    "\n",
    "\n",
    "optimization_result = minimize(\n",
    "    function_to_minimize,\n",
    "    x0=optimization_parameters.get_initial_guess,\n",
    "    args=(optimization_parameters.x2_fixed, optimization_parameters.y_target),\n",
    "    method=\"Powell\",\n",
    "    bounds=[optimization_parameters.bounds],\n",
    ")\n",
    "\n",
    "optimization_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d7f8e",
   "metadata": {},
   "source": [
    "Finally, we can visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.scatter(\n",
    "    x[:, 0][mask],\n",
    "    y[mask],\n",
    "    c=y[mask],\n",
    "    cmap=\"coolwarm\",\n",
    "    norm=norm,\n",
    "    edgecolors=\"black\",\n",
    "    label=rf\"data points close to $x_2 = {optimization_parameters.x2_fixed}$\",\n",
    ")\n",
    "ax.plot(\n",
    "    x1_grid,\n",
    "    y_pred_grid,\n",
    "    c=\"black\",\n",
    "    linewidth=3,\n",
    "    label=r\"predictions on a $x_1$-grid\",\n",
    ")\n",
    "ax.scatter(\n",
    "    optimization_result.x,\n",
    "    optimization_result.fun + optimization_parameters.y_target,\n",
    "    c=\"C2\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"black\",\n",
    "    s=200,\n",
    "    label=r\"optimal $x_1$\",\n",
    ")\n",
    "ax.axhline(\n",
    "    optimization_parameters.y_target,\n",
    "    color=\"C2\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    linewidth=3,\n",
    "    label=r\"target\",\n",
    ")\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set(xlabel=r\"$x_1$\", ylabel=r\"$y$\", title=r\"$x_1$ vs $y$\", xlim=(0.15, 0.65))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe7bf6",
   "metadata": {},
   "source": [
    "We can see that the optimal $x_1$ does hit the target as expected ðŸš€!\n",
    "\n",
    "We can explicitly plot the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_grid = np.vectorize(function_to_minimize)(\n",
    "    x1_grid, optimization_parameters.x2_fixed, optimization_parameters.y_target\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x1_grid, objective_grid, c=\"C0\", linewidth=3, label=\"objective\")\n",
    "ax.scatter(\n",
    "    optimization_result.x,\n",
    "    optimization_result.fun,\n",
    "    c=\"C2\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"black\",\n",
    "    s=200,\n",
    "    label=r\"optimal $x_1$\",\n",
    ")\n",
    "ax.axhline(\n",
    "    0.0,\n",
    "    color=\"C2\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    linewidth=3,\n",
    "    label=\"zero objective\",\n",
    ")\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set(\n",
    "    xlabel=r\"$x_1$\",\n",
    "    ylabel=r\"$ |\\text{ML}(x) - y_{target}|$\",\n",
    "    title=\"Objective function\",\n",
    "    xlim=(0.15, 0.65),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b709de",
   "metadata": {},
   "source": [
    "## Final Remarks\n",
    "\n",
    "Although this is a simple example, it shows the general idea of using machine learning models for optimization purposes. Here are some additional comments:\n",
    "\n",
    "- For this model to work in practice, it has to have a *causal structure*. This is key because we do not want to make decisions solely based on the predictions of the model, which essentially models correlations and associations. See for example the blog post [\"Be Careful When Interpreting Predictive Models in Search of Causal Insights\"](https://medium.com/data-science/be-careful-when-interpreting-predictive-models-in-search-of-causal-insights-e68626e664b6). Hence, please write the DAG and ensure your counterfactual is identifiable.\n",
    "\n",
    "- We are consciously using a brute force approach with this gradient-free optimization. This does not mean it is bad, but there are other alternatives. For example, in the bidding paper, in *Algorithm 1 Algorithm BiCB*, they suggest using the projected gradient descent algorithm. This approach requires taking derivatives, so there is some additional work to be done.\n",
    "\n",
    "- This brute force approach might not scale well if you want to do this for millions of time series (e.g. bids). It might be that the gradient descent approach mentioned above is more suitable for this case.\n",
    "\n",
    "Besides those caveats, I found this little exercise very useful to get intuition on these methods ðŸ™ƒ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
