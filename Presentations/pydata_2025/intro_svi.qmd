---
title: "Scaling Probabilistic Models with Variational Inference"
title-slide-attributes:
  data-background-image: intro_svi_files/static/images/logos/pydata_berlin_logo.png
  data-background-size: cover
  data-background-opacity: "0.15"
subtitle: "PyData Berlin 2025"
author: 
  - name: Dr. Juan Orduz
    url: https://juanitorduz.github.io/

format:
  revealjs:
    slide-number: true
    html-math-method: mathjax 
    css: intro_svi_files/style.css
    logo: intro_svi_files/static/images/logos/pydata_berlin_logo.png 
    transition: none
    chalkboard: 
      buttons: false
    preview-links: auto
    theme:
        - white
    highlight-style: github-dark
---

## Outline

- Motivating Examples:
  - Custom Probabilistic Forecasting Models
  - Hierarchical Cohort Revenue-Retention Models

- Variational Inference in a Nutshell

- **End-to-End Example: Bayesian Neural Network Model**

- References


::: {.callout-tip}
## Code
  
[https://juanitorduz.github.io/intro_svi/](https://juanitorduz.github.io/intro_svi/)
:::


## Toy Example: Two Moons Dataset

Classification problem with non-linear decision boundary.

![](intro_svi_files/static/images/intro_svi_files/intro_svi_6_0.png){fig-align="center" width="1000"}

::: footer
[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)
:::

## Model Structure

::: {.callout-tip}
## Model Components
  
- We use a multi-layer perceptron (MLP) to model the non-linear decision boundary parameter $p(x)$.
- We model our data with a Bernoulli likelihood with parameter $p(x)$.

:::


![](intro_svi_files/static/images/intro_svi_files/intro_svi_15_1.svg){fig-align="center" width="1000"}

## Neural Network Component

```{.python code-line-numbers="|1-3|5-10|12-15|17-21"}
from itertools import pairwise
import jax
from flax import nnx

class MLP(nnx.Module):

    def __init__(
        self, din: int, dout: int, hidden_layers: list[int], *, rngs: nnx.Rngs
    ) -> None:
        self.layers = []

        layer_dims = [din, *hidden_layers, dout]

        for in_dim, out_dim in pairwise(layer_dims):
            self.layers.append(nnx.Linear(in_dim, out_dim, rngs=rngs))

    def __call__(self, x: jax.Array) -> jax.Array:
        for layer in self.layers[:-1]:
            x = jax.nn.tanh(layer(x))

        return jax.nn.sigmoid(self.layers[-1](x))
```

## Initialize the Neural Network

```{.python code-line-numbers="|1-3|5-6|8-13"}
import jax.random as random

rng_key, rng_subkey = random.split(rng_key)

hidden_layers = [4, 3]
dout = 1

nnx_module = MLP(
    din=x_train.shape[1],
    dout=dout,
    hidden_layers=hidden_layers,
    rngs=nnx.Rngs(rng_subkey),
)
```

## NumPyro Model

```{.python code-line-numbers="|1-3|5-8|9-14|16-20|22|24-25"}
import numpyro
import numpyro.distributions as dist
from numpyro.contrib.module import random_nnx_module

def model(
    x: Float[Array, "n_obs features"],
    y: Int[Array, " n_obs"] | None = None,
) -> None:
    n_obs: int = x.shape[0]

    def prior(name, shape):
        if "bias" in name:
            return dist.Normal(loc=0, scale=1)
        return dist.SoftLaplace(loc=0, scale=1)

    nn = random_nnx_module(
        "nn",
        nnx_module,
        prior=prior,
    )

    p = numpyro.deterministic("p", nn(x).squeeze(-1))

    with numpyro.plate("data", n_obs):
        numpyro.sample("y", dist.Bernoulli(probs=p), obs=y)
```

## Prior Predictive Checks

![](intro_svi_files/static/images/intro_svi_files/intro_svi_20_0.png){fig-align="center" width="1000"}

## Inference: SVI in NumPyro

```{.python code-line-numbers="|5|6|7|8|10-17|19-20"}
import numpyro
from numpyro.infer import SVI, Trace_ELBO
from numpyro.infer.autoguide import AutoNormal

guide = AutoNormal(model) # <- Posterior Approximation
optimizer = numpyro.optim.Adam(step_size=0.01) # <- Optimizer
svi = SVI(model, guide, optimizer, loss=Trace_ELBO())
num_steps = 10_000

rng_key, rng_subkey = random.split(key=rng_key)

# Optimization Loop
svi_result = svi.run(
    rng_subkey,
    num_steps,
    x_train,
    y_train,
)

# Results
params = svi_result.params
```

## Inference: ELBO Curve

![](intro_svi_files/static/images/elbo_curve_example.png){fig-align="center" width="1000"}


## Inference: Custom Guide ðŸ¤•

```{.python code-line-numbers="|1-11|13-23|25-31|33-45"}
def layer_guide(
    loc_shape: tuple[int, ...],
    loc_amplitude: float,
    scale_shape: tuple[int, ...],
    scale_amplitude: float,
    loc_name: str,
    scale_name: str,
    layer_name: str,
    event_shape: int = 1,
    seed: int = 42,
) -> None:

    # Create local random key for this layer
    rng_key = random.PRNGKey(seed)

    # Initialize location parameters with random values
    rng_key, rng_subkey = random.split(rng_key)
    # As for general neural networks, we need to initialize the parameters.
    # It is recommended to initialize the parameters at random.
    # Here we do it for the location parameters.
    loc = numpyro.param(
        loc_name, loc_amplitude * random.uniform(rng_subkey, shape=loc_shape)
    )

    # We do the same for the scale parameters.
    rng_key, rng_subkey = random.split(rng_key)
    scale = numpyro.param(
        scale_name,
        scale_amplitude * random.uniform(rng_subkey, shape=scale_shape),
        constraint=dist.constraints.positive,  # Ensure scale > 0
    )

    # Choose distribution type based on layer name
    if "bias" in layer_name:
        # Bias parameters use Normal distribution (matching model prior)
        numpyro.sample(
            layer_name,
            dist.Normal(loc=loc, scale=scale).to_event(event_shape),
        )
    else:
        # Weight parameters use SoftLaplace distribution (matching model prior)
        numpyro.sample(
            layer_name,
            dist.SoftLaplace(loc=loc, scale=scale).to_event(event_shape),
        )

```


## Inference: Custom Optimizer

```{.python code-line-numbers="|1|3-10|11-21"}
import optax
# OneCycle schedule: low -> high -> low with specific timing
scheduler = optax.linear_onecycle_schedule(
    transition_steps=8_000,  # Total number of optimization steps
    peak_value=0.008,  # Maximum learning rate (reached at pct_start)
    pct_start=0.008,  # Percent of training to reach peak (0.8%)
    pct_final=0.8,  # Percent of training for final phase (80%)
    div_factor=3,  # Initial LR = peak_value / div_factor
    final_div_factor=4,  # Final LR = initial_LR / final_div_factor
)
# Chain multiple optimizers for sophisticated training
optimizer = optax.chain(
    # Primary optimizer: Adam with scheduled learning rate
    optax.adam(learning_rate=scheduler),
    # Secondary optimizer: Reduce LR when loss plateaus
    optax.contrib.reduce_on_plateau(
        factor=0.1,  # Multiply LR by 0.1 when plateau detected
        patience=10,  # Wait 10 evaluations before reducing
        accumulation_size=100,  # Window size for detecting plateaus
    ),
)
```

## Inference: Custom Optimization Loop

```{.python code-line-numbers="|1-3|5-7|9-11|13-22|24-32"}
def body_fn(svi_state, _):
    svi_state, loss = svi.update(svi_state, x=x_train, y=y_train)
    return svi_state, loss

with tqdm.trange(1, num_steps + 1) as t:
    batch = max(num_steps // 20, 1)
    patience_counter = 0

    for i in t:
        # Perform one training step (JIT compiled for speed)
        svi_state, train_loss = jax.jit(body_fn)(svi_state, None)

        # Normalize loss by dataset size for fair comparison
        norm_train_loss = jax.device_get(train_loss) / y_train.shape[0]
        train_losses.append(jax.device_get(train_loss))
        norm_train_losses.append(norm_train_loss)

        # Compute validation loss (JIT compiled for speed)
        val_loss = jax.jit(get_val_loss)(svi_state, x_val, y_val)
        norm_val_loss = jax.device_get(val_loss) / y_val.shape[0]
        val_losses.append(jax.device_get(val_loss))
        norm_val_losses.append(norm_val_loss)

        # Early stopping logic: stop if validation loss > training loss consistently
        condition = norm_val_loss > norm_train_loss
        patience_counter = patience_counter + 1 if condition else 0

        if patience_counter >= patience:
            print(
                f"Early stopping at step {i} (validation loss exceeding training loss)"
            )
            break
```


## Inference: Early Stopping

![](intro_svi_files/static/images/intro_svi_files/intro_svi_30_0.png){fig-align="center" width="1000"}

```{.python}
37%|â–ˆâ–ˆâ–ˆâ–‹      | 2924/8000 [00:02<00:04, 1145.56it/s, train: 396.8835, val: 202.2204]
Early stopping at step 2925 (validation loss exceeding training loss)
```

## Model Evaluation: Posterior AUC

![](intro_svi_files/static/images/intro_svi_files/intro_svi_39_0.png){fig-align="center" width="1000"}

## Model Evaluation: Posterior ROC

![](intro_svi_files/static/images/intro_svi_files/intro_svi_47_0.png){fig-align="center" width="1000"}

## Posterior Predictive Mean

![](intro_svi_files/static/images/intro_svi_files/intro_svi_49_0.png){fig-align="center" width="1000"}

## Posterior Predictive *Uncertainty*

![](intro_svi_files/static/images/intro_svi_files/intro_svi_51_0.png){fig-align="center" width="1000"}

## Predictions Outside Training Data

![](intro_svi_files/static/images/intro_svi_files/intro_svi_56_0.png){fig-align="center" width="1000"}

## References